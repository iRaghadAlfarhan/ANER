{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "4YSvpD3A1BMQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget \"https://www.cs.cmu.edu/%7Eark/ArabicNER/AQMAR_Arabic_NER_corpus-1.0.zip\"\n",
        "! unzip \"/content/AQMAR_Arabic_NER_corpus-1.0.zip\" -d \"/content/corpus\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFqSDE2Xq6e-",
        "outputId": "47c5d81a-cdec-4987-aebb-d967452f128c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-15 23:52:55--  https://www.cs.cmu.edu/%7Eark/ArabicNER/AQMAR_Arabic_NER_corpus-1.0.zip\n",
            "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
            "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7815886 (7.5M) [application/zip]\n",
            "Saving to: ‘AQMAR_Arabic_NER_corpus-1.0.zip’\n",
            "\n",
            "AQMAR_Arabic_NER_co 100%[===================>]   7.45M  2.89MB/s    in 2.6s    \n",
            "\n",
            "2023-12-15 23:52:58 (2.89 MB/s) - ‘AQMAR_Arabic_NER_corpus-1.0.zip’ saved [7815886/7815886]\n",
            "\n",
            "Archive:  /content/AQMAR_Arabic_NER_corpus-1.0.zip\n",
            "  inflating: /content/corpus/Atom.txt  \n",
            "  inflating: /content/corpus/Christiano_Ronaldo.txt  \n",
            "  inflating: /content/corpus/Computer.txt  \n",
            "  inflating: /content/corpus/Computer_Software.txt  \n",
            "  inflating: /content/corpus/Crusades.txt  \n",
            "  inflating: /content/corpus/Damascus.txt  \n",
            "  inflating: /content/corpus/Enrico_Fermi.txt  \n",
            "  inflating: /content/corpus/Football.txt  \n",
            "  inflating: /content/corpus/Ibn_Tolun_Mosque.txt  \n",
            "  inflating: /content/corpus/Imam_Hussein_Shrine.txt  \n",
            "  inflating: /content/corpus/Internet.txt  \n",
            "  inflating: /content/corpus/Islamic_Golden_Age.txt  \n",
            "  inflating: /content/corpus/Islamic_History.txt  \n",
            "  inflating: /content/corpus/LICENSE  \n",
            "  inflating: /content/corpus/Light.txt  \n",
            "  inflating: /content/corpus/Linux.txt  \n",
            "  inflating: /content/corpus/Nuclear_Power.txt  \n",
            "  inflating: /content/corpus/Periodic_Table.txt  \n",
            "  inflating: /content/corpus/Physics.txt  \n",
            "  inflating: /content/corpus/Portugal_football_team.txt  \n",
            "  inflating: /content/corpus/README  \n",
            "  inflating: /content/corpus/Raul_Gonzales.txt  \n",
            "  inflating: /content/corpus/Razi.txt  \n",
            "  inflating: /content/corpus/Real_Madrid.txt  \n",
            "  inflating: /content/corpus/Richard_Stallman.txt  \n",
            "  inflating: /content/corpus/Soccer_Worldcup.txt  \n",
            "  inflating: /content/corpus/Solaris.txt  \n",
            "  inflating: /content/corpus/Summer_Olympics2004.txt  \n",
            "  inflating: /content/corpus/Ummaya_Mosque.txt  \n",
            "  inflating: /content/corpus/VERSION  \n",
            "  inflating: /content/corpus/X_window_system.txt  \n",
            "   creating: /content/corpus/featureFiles/\n",
            "   creating: /content/corpus/featureFiles/.svn/\n",
            "  inflating: /content/corpus/featureFiles/.svn/entries  \n",
            "   creating: /content/corpus/featureFiles/.svn/prop-base/\n",
            "   creating: /content/corpus/featureFiles/.svn/props/\n",
            "   creating: /content/corpus/featureFiles/.svn/text-base/\n",
            "  inflating: /content/corpus/featureFiles/.svn/text-base/README.svn-base  \n",
            "   creating: /content/corpus/featureFiles/.svn/tmp/\n",
            "   creating: /content/corpus/featureFiles/.svn/tmp/prop-base/\n",
            "   creating: /content/corpus/featureFiles/.svn/tmp/props/\n",
            "   creating: /content/corpus/featureFiles/.svn/tmp/text-base/\n",
            "   creating: /content/corpus/featureFiles/dev/\n",
            "   creating: /content/corpus/featureFiles/dev/.svn/\n",
            "  inflating: /content/corpus/featureFiles/dev/.svn/entries  \n",
            "   creating: /content/corpus/featureFiles/dev/.svn/prop-base/\n",
            "   creating: /content/corpus/featureFiles/dev/.svn/props/\n",
            "   creating: /content/corpus/featureFiles/dev/.svn/text-base/\n",
            "  inflating: /content/corpus/featureFiles/dev/.svn/text-base/all.dev.features.txt.svn-base  \n",
            "  inflating: /content/corpus/featureFiles/dev/.svn/text-base/history.dev.features.txt.svn-base  \n",
            "  inflating: /content/corpus/featureFiles/dev/.svn/text-base/science.dev.features.txt.svn-base  \n",
            "  inflating: /content/corpus/featureFiles/dev/.svn/text-base/sport.dev.features.txt.svn-base  \n",
            "  inflating: /content/corpus/featureFiles/dev/.svn/text-base/tech.dev.features.txt.svn-base  \n",
            "   creating: /content/corpus/featureFiles/dev/.svn/tmp/\n",
            "   creating: /content/corpus/featureFiles/dev/.svn/tmp/prop-base/\n",
            "   creating: /content/corpus/featureFiles/dev/.svn/tmp/props/\n",
            "   creating: /content/corpus/featureFiles/dev/.svn/tmp/text-base/\n",
            "  inflating: /content/corpus/featureFiles/dev/all.dev.features.txt  \n",
            "  inflating: /content/corpus/featureFiles/dev/history.dev.features.txt  \n",
            "  inflating: /content/corpus/featureFiles/dev/science.dev.features.txt  \n",
            "  inflating: /content/corpus/featureFiles/dev/sport.dev.features.txt  \n",
            "  inflating: /content/corpus/featureFiles/dev/tech.dev.features.txt  \n",
            "   creating: /content/corpus/featureFiles/lexicons/\n",
            "   creating: /content/corpus/featureFiles/lexicons/.svn/\n",
            "  inflating: /content/corpus/featureFiles/lexicons/.svn/entries  \n",
            "   creating: /content/corpus/featureFiles/lexicons/.svn/prop-base/\n",
            " extracting: /content/corpus/featureFiles/lexicons/.svn/prop-base/NEList.txt.svn-base  \n",
            " extracting: /content/corpus/featureFiles/lexicons/.svn/prop-base/NonNEList.txt.svn-base  \n",
            "   creating: /content/corpus/featureFiles/lexicons/.svn/props/\n",
            "   creating: /content/corpus/featureFiles/lexicons/.svn/text-base/\n",
            "  inflating: /content/corpus/featureFiles/lexicons/.svn/text-base/NEList.txt.svn-base  \n",
            "  inflating: /content/corpus/featureFiles/lexicons/.svn/text-base/NonNEList.txt.svn-base  \n",
            "   creating: /content/corpus/featureFiles/lexicons/.svn/tmp/\n",
            "   creating: /content/corpus/featureFiles/lexicons/.svn/tmp/prop-base/\n",
            "   creating: /content/corpus/featureFiles/lexicons/.svn/tmp/props/\n",
            "   creating: /content/corpus/featureFiles/lexicons/.svn/tmp/text-base/\n",
            "  inflating: /content/corpus/featureFiles/lexicons/NEList.txt  \n",
            "  inflating: /content/corpus/featureFiles/lexicons/NonNEList.txt  \n",
            "  inflating: /content/corpus/featureFiles/README  \n",
            "   creating: /content/corpus/featureFiles/test/\n",
            "   creating: /content/corpus/featureFiles/test/.svn/\n",
            "  inflating: /content/corpus/featureFiles/test/.svn/entries  \n",
            "   creating: /content/corpus/featureFiles/test/.svn/prop-base/\n",
            "   creating: /content/corpus/featureFiles/test/.svn/props/\n",
            "   creating: /content/corpus/featureFiles/test/.svn/text-base/\n",
            "  inflating: /content/corpus/featureFiles/test/.svn/text-base/all.test.features.txt.svn-base  \n",
            "  inflating: /content/corpus/featureFiles/test/.svn/text-base/history.test.features.txt.svn-base  \n",
            "  inflating: /content/corpus/featureFiles/test/.svn/text-base/science.test.features.txt.svn-base  \n",
            "  inflating: /content/corpus/featureFiles/test/.svn/text-base/sport.test.features.txt.svn-base  \n",
            "  inflating: /content/corpus/featureFiles/test/.svn/text-base/tech.test.features.txt.svn-base  \n",
            "   creating: /content/corpus/featureFiles/test/.svn/tmp/\n",
            "   creating: /content/corpus/featureFiles/test/.svn/tmp/prop-base/\n",
            "   creating: /content/corpus/featureFiles/test/.svn/tmp/props/\n",
            "   creating: /content/corpus/featureFiles/test/.svn/tmp/text-base/\n",
            "  inflating: /content/corpus/featureFiles/test/all.test.features.txt  \n",
            "  inflating: /content/corpus/featureFiles/test/history.test.features.txt  \n",
            "  inflating: /content/corpus/featureFiles/test/science.test.features.txt  \n",
            "  inflating: /content/corpus/featureFiles/test/sport.test.features.txt  \n",
            "  inflating: /content/corpus/featureFiles/test/tech.test.features.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entity Cleaner: Unites entity tags and fixs misspellings\n",
        "def tags_cleaner(entity):\n",
        "  entity = re.sub('\\n','',entity) # Remove the newline (\\n)\n",
        "  if entity in ['B-LOC', 'B-MIS', 'B-ORG','B-PER','I-LOC','I-MIS','I-ORG','I-PER','O']:\n",
        "    return entity\n",
        "  elif entity in ['B-MIS0','B-MIS1', 'B-MIS2', 'B-MIS3', 'B-MIS-1','B-MIS-2', 'B-MIS1`', 'B-MISS1']:\n",
        "    return 'B-MIS'\n",
        "  elif entity in ['I-MIS0','I-MIS1', 'I-MIS2', 'I-MIS3']:\n",
        "    return 'I-MIS'\n",
        "  elif entity in ['B-ENGLISH', 'B-SPANISH', 'OO', 'IO']:\n",
        "    return 'O'\n",
        "  elif entity == 'I--ORG':\n",
        "    return 'I-ORG'\n",
        "  else:\n",
        "    print('Error with entity:', entity)\n",
        "\n",
        "\n",
        "# Clean/Normalize Arabic Text\n",
        "def clean_str(text):\n",
        "    search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
        "    replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ',' ! ']\n",
        "\n",
        "    # Remove tashkeel\n",
        "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
        "    text = re.sub(p_tashkeel,\"\", text)\n",
        "\n",
        "    # Remove longation\n",
        "    p_longation = re.compile(r'(.)\\1+')\n",
        "    subst = r\"\\1\\1\"\n",
        "    text = re.sub(p_longation, subst, text)\n",
        "\n",
        "    text = text.replace('وو', 'و')\n",
        "    text = text.replace('يي', 'ي')\n",
        "    text = text.replace('اا', 'ا')\n",
        "\n",
        "    for i in range(0, len(search)):\n",
        "        text = text.replace(search[i], replace[i])\n",
        "\n",
        "    # Trim\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Remove empty strings or strings that contains spaces only from sentences\n",
        "def re_clean(old_sentence, old_tags):\n",
        "  space_regex = re.compile(\"\\s+\")\n",
        "  new_sentence = []\n",
        "  new_tags = []\n",
        "  for j in range(len(old_sentence)):\n",
        "    # add word if not empty and doesn't contain spaces only\n",
        "    if old_sentence[j]!=\"\" and space_regex.match(old_sentence[j])==None:\n",
        "      new_sentence.append(old_sentence[j])\n",
        "      new_tags.append(old_tags[j])\n",
        "\n",
        "  return new_sentence, new_tags\n"
      ],
      "metadata": {
        "id": "lHyhIIF0ofT1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read sentences\n",
        "sentences = []\n",
        "tags = []\n",
        "vocab = set()\n",
        "\n",
        "corpus_path = \"/content/corpus/\"\n",
        "for file in os.listdir(corpus_path):\n",
        "  if file.endswith('.txt'): # Get txt files only\n",
        "    print('Processing:', file)\n",
        "    topic = open(corpus_path+file)\n",
        "    sentence = []\n",
        "    entity = []\n",
        "    for line in topic.readlines():\n",
        "      if line == '\\n': # Sentence end\n",
        "        recleaned = re_clean(sentence, entity)\n",
        "        sentences.append(recleaned[0].copy())\n",
        "        tags.append(recleaned[1].copy())\n",
        "        sentence.clear()\n",
        "        entity.clear()\n",
        "      else:\n",
        "        line = line.split(sep=' ')\n",
        "        clean_word = clean_str(line[0])       # Cleaning word\n",
        "        vocab.add(clean_word)                 # Add word to the vocab\n",
        "        sentence.append(clean_word)           # Add the word\n",
        "        entity.append(tags_cleaner(line[1]))  # Clean and add entity\n",
        "\n",
        "\n",
        "print('Done [Sentences:', len(sentences), ', Tags:', len(tags), ', Unique Words:', len(vocab))"
      ],
      "metadata": {
        "id": "uInDRuaERS4K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27b575ec-5a58-4c33-fec0-097addfcb55d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: Islamic_History.txt\n",
            "Processing: Ummaya_Mosque.txt\n",
            "Processing: Richard_Stallman.txt\n",
            "Processing: Computer_Software.txt\n",
            "Processing: X_window_system.txt\n",
            "Processing: Damascus.txt\n",
            "Processing: Summer_Olympics2004.txt\n",
            "Processing: Christiano_Ronaldo.txt\n",
            "Processing: Imam_Hussein_Shrine.txt\n",
            "Processing: Periodic_Table.txt\n",
            "Processing: Solaris.txt\n",
            "Processing: Ibn_Tolun_Mosque.txt\n",
            "Processing: Real_Madrid.txt\n",
            "Processing: Linux.txt\n",
            "Processing: Computer.txt\n",
            "Processing: Raul_Gonzales.txt\n",
            "Processing: Football.txt\n",
            "Processing: Razi.txt\n",
            "Processing: Portugal_football_team.txt\n",
            "Processing: Nuclear_Power.txt\n",
            "Processing: Islamic_Golden_Age.txt\n",
            "Processing: Atom.txt\n",
            "Processing: Light.txt\n",
            "Processing: Crusades.txt\n",
            "Processing: Soccer_Worldcup.txt\n",
            "Processing: Physics.txt\n",
            "Processing: Internet.txt\n",
            "Processing: Enrico_Fermi.txt\n",
            "Done [Sentences: 2687 , Tags: 2687 , Unique Words: 17481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a mapping betwween words and their IDs\n",
        "word2id = {word:id for  id, word in enumerate(vocab)}\n",
        "id2word = {id:word for  id, word in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "T4EKWDM599h9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Sentence encoder\n",
        "def encode_sentence(old_sentence):\n",
        "  encoded_sentence = []\n",
        "  for word in old_sentence:\n",
        "    try:\n",
        "      encoded_sentence.append(word2id[word])\n",
        "    except KeyError:\n",
        "      encoded_sentence.append(0) # A dummy digit for out of vocab\n",
        "\n",
        "  return encoded_sentence\n",
        "\n",
        "# Encode Tags\n",
        "tags_encoding = {\n",
        "    'B-LOC':0,\n",
        "    'B-MIS':1,\n",
        "    'B-ORG':2,\n",
        "    'B-PER':3,\n",
        "    'I-LOC':4,\n",
        "    'I-MIS':5,\n",
        "    'I-ORG':6,\n",
        "    'I-PER':7,\n",
        "    'O':8\n",
        "  }\n",
        "def encode_tags(old_tags):\n",
        "  new_tags = [tags_encoding[tag] for tag in old_tags]\n",
        "  new_tags = to_categorical(y = new_tags, num_classes=9)\n",
        "  return new_tags"
      ],
      "metadata": {
        "id": "9B0jdBD9-a9o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding\n",
        "sentences_encoded = []\n",
        "tags_encoded = []\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  sentences_encoded.append(encode_sentence(sentences[i]))\n",
        "  tags_encoded.append(encode_tags(tags[i]))"
      ],
      "metadata": {
        "id": "8p6QS6azAFp3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Padding\n",
        "MAX_SEQUENCE_LENGTH = 40\n",
        "\n",
        "sentences_padded = pad_sequences(sequences = sentences_encoded,\n",
        "                                 maxlen=MAX_SEQUENCE_LENGTH,\n",
        "                                 dtype='int32',\n",
        "                                 padding='post',\n",
        "                                 truncating='post',\n",
        "                                 value = 0)\n",
        "tags_padded = pad_sequences(sequences = tags_encoded,\n",
        "                                 maxlen=MAX_SEQUENCE_LENGTH,\n",
        "                                 dtype='int32',\n",
        "                                 padding='post',\n",
        "                                 truncating='post',\n",
        "                                 value = np.array([0., 0., 0., 0., 0., 0., 0., 0., 1.]))"
      ],
      "metadata": {
        "id": "kKcTVg3uB84a"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting data\n",
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(sentences_padded,\n",
        "                                                                              tags_padded,\n",
        "                                                                              train_size=0.8,\n",
        "                                                                              random_state=42)"
      ],
      "metadata": {
        "id": "n3io8aspRJTK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download AraVec (Word2Vec Model) by Abu Bakr Soliman, Kareem Eissa, and Samhaa R.El-Beltagy.\n",
        "! wget \"https://archive.org/download/aravec2.0/wiki_cbow_300.zip\"\n",
        "! unzip \"/content/wiki_cbow_300.zip\" -d \"/content/word2vec_model\""
      ],
      "metadata": {
        "id": "oFvpL__UI6Qk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ec4fe17-24db-45fc-9c74-2a68afb5dcf5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-15 23:53:05--  https://archive.org/download/aravec2.0/wiki_cbow_300.zip\n",
            "Resolving archive.org (archive.org)... 207.241.224.2\n",
            "Connecting to archive.org (archive.org)|207.241.224.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ia803107.us.archive.org/0/items/aravec2.0/wiki_cbow_300.zip [following]\n",
            "--2023-12-15 23:53:05--  https://ia803107.us.archive.org/0/items/aravec2.0/wiki_cbow_300.zip\n",
            "Resolving ia803107.us.archive.org (ia803107.us.archive.org)... 207.241.232.157\n",
            "Connecting to ia803107.us.archive.org (ia803107.us.archive.org)|207.241.232.157|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 364888893 (348M) [application/zip]\n",
            "Saving to: ‘wiki_cbow_300.zip’\n",
            "\n",
            "wiki_cbow_300.zip   100%[===================>] 347.98M  1.70MB/s    in 84s     \n",
            "\n",
            "2023-12-15 23:54:29 (4.14 MB/s) - ‘wiki_cbow_300.zip’ saved [364888893/364888893]\n",
            "\n",
            "Archive:  /content/wiki_cbow_300.zip\n",
            "  inflating: /content/word2vec_model/wikipedia_cbow_300  \n",
            "  inflating: /content/word2vec_model/wikipedia_cbow_300.trainables.syn1neg.npy  \n",
            "  inflating: /content/word2vec_model/wikipedia_cbow_300.wv.vectors.npy  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "\n",
        "# Load the Word2Vec model\n",
        "weights_path = \"/content/word2vec_model/wikipedia_cbow_300\"\n",
        "araVec = gensim.models.Word2Vec.load(weights_path)\n",
        "\n",
        "# Testing\n",
        "most_similar = araVec.wv.most_similar( \"محمد\" )\n",
        "for term, score in most_similar:\n",
        "\tprint(term, score)"
      ],
      "metadata": {
        "id": "RaMr0gIxvpus",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c3ef65f-bd45-45c6-aa78-cdfdcc7e9c52"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "لمحمد 0.726012110710144\n",
            "احمد 0.7142193913459778\n",
            "عبدالرحمن 0.6745273470878601\n",
            "ابراهيم 0.6723851561546326\n",
            "مهدي 0.6686975955963135\n",
            "محمود 0.6648465991020203\n",
            "يحي 0.637116551399231\n",
            "اسماعيل 0.6307213306427002\n",
            "حموده 0.6287057995796204\n",
            "عبدالحميد 0.6267550587654114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_words = len(vocab)\n",
        "embed_size = araVec.wv.vector_size\n",
        "embedding_matrix = np.zeros(shape=(num_words, embed_size))\n",
        "\n",
        "for word, id in word2id.items():\n",
        "    try:\n",
        "        embedding_matrix[id] = araVec.wv[word]\n",
        "    except KeyError:\n",
        "        embedding_matrix[id] = np.zeros(embed_size)\n",
        "\n",
        "embedding_matrix.shape"
      ],
      "metadata": {
        "id": "MtwcA63dOEYR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81d4e32c-264d-4936-ca97-f9a5e1760b41"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17481, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import  Input, Dense, Embedding, TimeDistributed ,GRU , Bidirectional\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "\n",
        "tf.keras.backend.clear_session() # Makes sure old model was deleted if exists\n",
        "\n",
        "GRU_model = Sequential()\n",
        "# Adding Layers\n",
        "GRU_model.add(Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32'))\n",
        "GRU_model.add(Embedding(input_dim = len(vocab),              # Vocabulary Size (number of unique words for training)\n",
        "                        output_dim = embed_size,              # Length of the vector for each word (embedding dimension)\n",
        "                        input_length = MAX_SEQUENCE_LENGTH,   # Maximum length of a sequence\n",
        "                        weights = [embedding_matrix],         # Send the needed AraVec Weights\n",
        "                        trainable = False))\n",
        "\n",
        "GRU_model.add(Bidirectional(GRU(10, return_sequences=True)))\n",
        "GRU_model.add(TimeDistributed(Dense(9, activation='softmax')))\n",
        "\n",
        "# Compile the model\n",
        "GRU_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999),\n",
        "                   loss='categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "GRU_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTtLQW_3Npb4",
        "outputId": "4a308c04-9e46-45d6-9620-110e07e6d14b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 40, 300)           5244300   \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 40, 20)            18720     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " time_distributed (TimeDist  (None, 40, 9)             189       \n",
            " ributed)                                                        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5263209 (20.08 MB)\n",
            "Trainable params: 18909 (73.86 KB)\n",
            "Non-trainable params: 5244300 (20.01 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GRU_model.fit(train_sentences,\n",
        "               train_labels,\n",
        "               validation_split=0.15,\n",
        "               batch_size = 10,\n",
        "               epochs = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmUcYDjbVVP-",
        "outputId": "9c0d32ff-6b8f-4d99-bcad-45ff705b014d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "183/183 [==============================] - 10s 12ms/step - loss: 0.4941 - accuracy: 0.8891 - val_loss: 0.2826 - val_accuracy: 0.9318\n",
            "Epoch 2/10\n",
            "183/183 [==============================] - 2s 8ms/step - loss: 0.2523 - accuracy: 0.9355 - val_loss: 0.2301 - val_accuracy: 0.9409\n",
            "Epoch 3/10\n",
            "183/183 [==============================] - 2s 8ms/step - loss: 0.2091 - accuracy: 0.9425 - val_loss: 0.2023 - val_accuracy: 0.9443\n",
            "Epoch 4/10\n",
            "183/183 [==============================] - 2s 10ms/step - loss: 0.1831 - accuracy: 0.9463 - val_loss: 0.1853 - val_accuracy: 0.9468\n",
            "Epoch 5/10\n",
            "183/183 [==============================] - 2s 11ms/step - loss: 0.1653 - accuracy: 0.9504 - val_loss: 0.1732 - val_accuracy: 0.9488\n",
            "Epoch 6/10\n",
            "183/183 [==============================] - 2s 8ms/step - loss: 0.1517 - accuracy: 0.9539 - val_loss: 0.1644 - val_accuracy: 0.9522\n",
            "Epoch 7/10\n",
            "183/183 [==============================] - 1s 8ms/step - loss: 0.1402 - accuracy: 0.9567 - val_loss: 0.1577 - val_accuracy: 0.9539\n",
            "Epoch 8/10\n",
            "183/183 [==============================] - 1s 8ms/step - loss: 0.1301 - accuracy: 0.9598 - val_loss: 0.1536 - val_accuracy: 0.9539\n",
            "Epoch 9/10\n",
            "183/183 [==============================] - 1s 8ms/step - loss: 0.1217 - accuracy: 0.9620 - val_loss: 0.1480 - val_accuracy: 0.9556\n",
            "Epoch 10/10\n",
            "183/183 [==============================] - 2s 8ms/step - loss: 0.1137 - accuracy: 0.9639 - val_loss: 0.1469 - val_accuracy: 0.9553\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7828fc7fc760>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GRU_model.evaluate(test_sentences, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiB21M7bY7bW",
        "outputId": "b1feb557-d328-4358-9a1e-0d432397c30d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17/17 [==============================] - 0s 5ms/step - loss: 0.1387 - accuracy: 0.9559\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.13867048919200897, 0.9558550119400024]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def GRU_predict(sentence:str):\n",
        "  sentence = sentence.split(sep=' ')\n",
        "  # Keeping track of words so not to process 40 words every time\n",
        "  word_count = len(sentence)\n",
        "  # Clean sentence\n",
        "  ready_sentence = [clean_str(word) for word in sentence]\n",
        "  # Encode sentence\n",
        "  ready_sentence = encode_sentence(ready_sentence)\n",
        "  # Padding sentence\n",
        "  ready_sentence = pad_sequences(sequences = [ready_sentence],\n",
        "                                 maxlen=MAX_SEQUENCE_LENGTH,\n",
        "                                 dtype='int32',\n",
        "                                 padding='post',\n",
        "                                 truncating='post',\n",
        "                                 value = 0)\n",
        "  tag_classes = ['B-LOC', 'B-MIS', 'B-ORG', 'B-PER', 'I-LOC', 'I-MIS', 'I-ORG', 'I-PER', 'O']\n",
        "  # Predict and return actual words only\n",
        "  predictions = GRU_model.predict(ready_sentence)\n",
        "\n",
        "  from terminaltables import AsciiTable\n",
        "  table_data = [['word', 'prediction']]\n",
        "  for i, word in enumerate(sentence):\n",
        "      table_data.append([word, tag_classes[np.argmax(predictions[0][i])]])\n",
        "  table = AsciiTable(table_data)\n",
        "  print(table.table)\n",
        "\n"
      ],
      "metadata": {
        "id": "RZE4vqQ1t0r1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GRU_predict('منشئ المسجد هو أحمد بن طولون مؤسس الدولة الطولونية في مصر والشام، تعود أصوله إلى قبيلة التغزغز التركية، وكانت أُسرته تقيم في بخاري.')"
      ],
      "metadata": {
        "id": "J37iOR2l1J8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3a717d9-5c1d-415e-bfe0-cdecc46abd4a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 74ms/step\n",
            "+-----------+------------+\n",
            "| word      | prediction |\n",
            "+-----------+------------+\n",
            "| منشئ      | O          |\n",
            "| المسجد    | O          |\n",
            "| هو        | O          |\n",
            "| أحمد      | B-PER      |\n",
            "| بن        | I-PER      |\n",
            "| طولون     | I-PER      |\n",
            "| مؤسس      | I-PER      |\n",
            "| الدولة    | O          |\n",
            "| الطولونية | O          |\n",
            "| في        | O          |\n",
            "| مصر       | O          |\n",
            "| والشام،   | B-LOC      |\n",
            "| تعود      | O          |\n",
            "| أصوله     | O          |\n",
            "| إلى       | O          |\n",
            "| قبيلة     | O          |\n",
            "| التغزغز   | O          |\n",
            "| التركية،  | O          |\n",
            "| وكانت     | O          |\n",
            "| أُسرته    | O          |\n",
            "| تقيم      | O          |\n",
            "| في        | O          |\n",
            "| بخاري.    | B-LOC      |\n",
            "+-----------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GRU_predict('محمود حسام ذهب الي مسجد')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg5UYVd3EqBb",
        "outputId": "dd4a399a-9de6-4b8a-986d-2f65066c75bf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 71ms/step\n",
            "+-------+------------+\n",
            "| word  | prediction |\n",
            "+-------+------------+\n",
            "| محمود | B-PER      |\n",
            "| حسام  | I-PER      |\n",
            "| ذهب   | O          |\n",
            "| الي   | O          |\n",
            "| مسجد  | O          |\n",
            "+-------+------------+\n"
          ]
        }
      ]
    }
  ]
}